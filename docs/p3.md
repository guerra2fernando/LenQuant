# Phase 3 — Learning & Mutation Engine (Adaptive Quant Intelligence)

**Objective:** Make the system self-improving. It learns why winners win, adapts parameter distributions, and tunes allocation between strategies automatically.

## 1 — Goals

- Aggregate cross-experiment knowledge into a meta-model that predicts expected fitness given strategy DNA.
- Apply Bayesian optimization / reinforcement learning to generate new promising DNA.
- Reallocate virtual capital among strategies dynamically.
- Detect overfitting & trigger re-evaluation.
- Extend the Next.js dashboard with a Phase 3 "Learning Insights" workspace that visualizes meta-model outputs, allocator decisions, and overfitting alerts with Tailwind + shadcn (light/dark ready).

## 2 — New components

| Module | Purpose |
|--------|---------|
| `learning/meta_model.py` | RandomForest-based meta-model training & inference |
| `learning/bayes_optimizer.py` | Optuna-driven genome search leveraging meta-model scores |
| `learning/allocator.py` | cvxpy portfolio allocator with risk controls |
| `learning/loop.py` | Daily learning cycle orchestration + knowledge summaries |
| `monitor/overfit_detector.py` | Walk-forward decay detector for overfitting |
| `knowledge/repository.py` | Persist learning insights for assistant + UI |

## 3 — Meta-model design

- **Feature space** = flattened genome parameters + latest performance stats.
- **Target** = next-period ROI or Sharpe.
- Train with `sklearn.ensemble.RandomForestRegressor` or lightgbm.

```python
X = [flatten(genome), performance_features]
y = next_period_roi
model.fit(X, y)
```

Predict expected fitness for new genomes → select top k.

Store model artifact `meta_model_v1.joblib`.

## 4 — Bayesian optimization loop

Use `optuna` or `scikit-optimize`.

**Objective function:**

```python
def objective(params):
    genome = build_genome(params)
    score = run_quick_backtest(genome)
    return -score['neg_fitness']
```

Run sequential search with prior knowledge from meta_model.

## 5 — Capital allocator

**Input:** current live portfolio of strategies with fitness + risk.

Optimize weights subject to:

- `Σ wᵢ = 1`
- `max risk ≤ R_max`
- `diversity constraint (correlation ≤ ρ_max)`

### Implementation ideas:

```python
import cvxpy as cp
w = cp.Variable(n)
ret = expected_returns @ w
risk = cp.quad_form(w, cov_matrix)
prob = cp.Problem(cp.Maximize(ret - lambda_*risk), [cp.sum(w)==1, w>=0])
prob.solve()
```

Write weights to DB → used by simulator for virtual fund allocation.

## 6 — Overfitting detector

- Monitor fitness decay after promotion.
- If `decay > threshold` for N days, flag strategy as overfit.
- Auto-demote or retrain.

## 7 — Learning loop scheduler

Runs daily after leaderboard generation.

**Phases:**

1. Update meta-model with new data.
2. Generate candidate genomes (via Bayes opt or RL).
3. Reallocate virtual capital.
4. Flag overfit strategies.
5. Commit summary to report.

### Frontend scope (Phase 3 UI)

**New / Updated Pages**
- `/insights` (new) — Learning Insights Hub with tabs for Meta-Model, Allocator, Overfitting, Knowledge.
- `/settings/learning` (new) — Configure meta-model training cadence, allocator risk tolerance, overfit thresholds via `GET/PUT /api/settings/learning`.
- `/evolution` — Gains "Insights" sidebar summarizing allocator decisions and overfit warnings.

**Key Components**
- `InsightsTabs`, `FeatureImportanceHeatmap`, `GenomeTraitTable`.
- `AllocatorAllocationChart`, `AllocationDiffList`, `RiskScoreCard`.
- `OverfitAlertTable`, `AcknowledgeDialog`, `KnowledgeSummaryCard` (renders markdown).
- `SendToAssistantButton` bridging to Phase 4 assistant endpoint.

**Backend Integrations**
- Meta-model metrics: `GET /api/learning/meta-model`, `GET /api/learning/feature-importance`.
- Allocator outputs: `GET /api/learning/allocator`, `POST /api/learning/allocator/rebalance`.
- Overfitting signals: `GET /api/learning/overfit`, `POST /api/learning/overfit/ack`.
- Knowledge base: `GET /api/knowledge/latest`, `GET /api/knowledge/{period}`.
- Settings: `GET/PUT /api/settings/learning`.

**UX & QA**
- Chart components adhere to theme tokens and respond to tab changes without reflow glitches.
- `aria-live` notifications for allocator rebalances and overfit alerts.
- Playwright scenario: toggle theme, switch tabs, acknowledge overfit alert; expect persisted state.
- Storybook stories for `FeatureImportanceHeatmap`, `OverfitAlertTable`, `KnowledgeSummaryCard`.

## 8 — AI assistant enhancement

Enable natural-language queries like:

> "What patterns did the system learn from winning scalpers this week?"

Assistant queries `meta_model.feature_importances_` and synthesizes text.

## 9 — Tests

- Meta-model predicts fitness > random baseline.
- Allocator returns weights sum ≈ 1.
- Overfit detector flags simulated decay properly.

## 10 — Acceptance criteria

- ✅ Meta-model trained and stored.
- ✅ Bayesian optimizer produces new genomes daily.
- ✅ Capital allocator redistributes based on fitness automatically.
- ✅ Overfitting flags visible in dashboard + API.
- ✅ System learns incrementally from its own history.
- ✅ Learning Insights Hub frontend surfaces meta-model/allocator outputs and overfitting alerts with accessible light/dark styling.

## 11 — Implementation Status (Nov 10, 2025)

**Completed**
- `learning/meta_model.py` ingests historical `sim_runs`, flattens genome params + prior-run metrics, and trains a `RandomForestRegressor`. Artifacts (model + feature columns + metrics) are versioned under `learning/artifacts/` with metadata written to `learning.meta_models`.
- `learning/bayes_optimizer.py` wraps Optuna to mutate champion genomes, score candidates via the meta-model, and push the top genomes into the experiment queue (and `strategies` collection) with `bayes-opt` tagging.
- `learning/allocator.py` performs Markowitz-style optimisation using `cvxpy`, stores snapshots in `learning.allocations`, and exposes expected portfolio return/risk.
- `monitor/overfit_detector.py` evaluates walk-forward decay on recent runs, emitting alerts into `learning.overfit_alerts`, with acknowledgement support.
- `learning/loop.py` coordinates retraining, candidate generation, allocator rebalance, overfit detection, and knowledge base writes; exposed through `POST /api/learning/cycle/run` and script `scripts/run_learning_cycle.py`.
- Knowledge base persistence shipped via `knowledge/repository.py` plus REST surface `GET /api/knowledge/*`.

**API Surface**
- New FastAPI router `api/routes/learning.py` (`/api/learning/*`) delivers meta-model summaries, allocator snapshots, overfit alerts, learning settings (`GET/PUT`), and cycle execution.
- `api/routes/settings.py` now fronts `/api/settings/learning` for UI parity.

**Frontend**
- `web/next-app/pages/insights/index.tsx` introduces the Learning Insights hub with `InsightsTabs`, `FeatureImportanceHeatmap`, `AllocatorAllocationChart`, `OverfitAlertTable`, `KnowledgeSummaryCard`, etc.
- `/settings/learning` enables runtime editing of Phase 3 settings; `/evolution` sidebar now surfaces allocator focus + open overfit alerts.
- Navigation updated to include "Learning Insights" and link to learning settings.

**Tooling & Tests**
- Added unit coverage in `tests/test_learning_phase3.py` for feature-vector generation and overfit signalling helpers.
- Requirements updated with `optuna` and `cvxpy`; note that local pytest execution still depends on installing scientific stack (`scikit-learn`, `joblib`, etc.).

**Known gaps / next**
- RL agent remains a backlog item; current search is Bayesian-only.
- Allocator diff view currently compares against previous snapshot when available; long-term history charting is pending.
- Production deployment should ensure `cvxpy` solvers and Mongo indexes (`learning.*`) are provisioned, and Playwright smoke for `/insights` is still TODO.
