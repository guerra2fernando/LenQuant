# Phase 4 — Personal Assistant + Conversational Analysis (AI-driven UX & Explanations)

**Objective:** Build the user-facing assistant that explains system decisions, lets you interact conversationally with the lab, requests approvals, and issues actionable recommendations. This ties the research backend to a human-in-the-loop interface: chat, notifications, one-click approve/reject for suggested trades, and deep-dive explainability for strategies.

## 1 — High-level goals

- Conversational UI (chat) that answers questions about strategies, results, and diagnostics using stored run data and model outputs.
- Actionable recommendations for your personal account with explicit reasoning and risk estimates (explainable).
- Two interaction modes: **assistant chat** (informational + exploratory) and **trade flow** (recommend → approve/reject → execute simulated or live).
- LLM-backed explanations that combine factual evidence (plots, numbers) with concise natural language.
- Audit trail for every assistant statement, recommendation, and user decision.
- Deliver a polished assistant workspace in Next.js using Tailwind + shadcn with persistent light/dark theme toggle, responsive layouts, and accessible interactions.

## 2 — New components & services

| Component | Purpose |
|-----------|---------|
| `assistant/llm_worker.py` | Interface with LLM(s), handle prompt templates, retrieval + grounding |
| `assistant/retriever.py` | Fetch evidence: forecasts, trade logs, feature drifts, model evals |
| `assistant/explainer.py` | Generate human-readable diagnostics and concise explanations |
| `assistant/action_manager.py` | Create trade recommendation objects, capture approvals/denials |
| `ui/chat/` | Next.js chat UI + trade approval modals |
| `api/routes/assistant.py` | Endpoints for chat, recommended actions, approvals |
| `audit/logs` | Store conversation + evidence + decisions for compliance/replay |

## 3 — Design considerations & constraints

- **Grounded answers only:** LLM responses must be grounded in data pulled from retriever; never hallucinate trading facts or backtest results.
- **Explainability:** For every recommendation include required fields: `pred_return`, `confidence`, `recommended_size`, `stop_loss`, `rationale_summary`, `evidence_refs` (charts/trades/metrics).
- **Human-in-the-loop:** Default mode should be suggest-only. Auto-execution requires explicit opt-in and multi-layered guard rails.
- **Privacy:** LLMs may be hosted externally only if you accept sending data. Prefer local LLM or strict redaction before sending if you want to keep data private.

## 4 — Data flows & retrieval patterns

### 4.1 Typical chat query flow

1. User sends query: _"Why did ETH scalper fail yesterday?"_
2. `assistant.retriever` fetches relevant items (sim_runs for ETH, model predictions, feature drift statistics, execution logs).
3. `assistant.explainer` formats these into a short, factual context doc (100–300 tokens) with bullet evidence.
4. `assistant.llm_worker` uses a controlled prompt to synthesize explanation given the context doc (LLM only summarizes — no external browsing).
5. Response saved in `audit/logs` with evidence IDs.

### 4.2 Trade recommendation flow

1. System (daily or continuous) identifies candidate trades using ensemble forecasts and top genome strategies.
2. `assistant.action_manager` builds Recommendation object:

```json
{
  "rec_id":"rec-20251105-abc",
  "symbol":"ETH/USDT",
  "horizon":"1h",
  "pred_return":0.012,
  "confidence":0.73,
  "recommended_size_usd":500,
  "stop_loss_pct":0.015,
  "take_profit_pct":0.03,
  "rationale_summary":"Model ensemble predicts 1.2% next-hour return; volatility currently 0.6% vs historic 1.2% indicating favorable environment. Backtest analog shows 62% hit rate.",
  "evidence_refs":["sim_runs/.....","models/lgbm_1h_v4_eval.png"]
}
```

3. User can approve / modify / reject in chat or dashboard. All decisions logged.
4. If approved and auto-mode enabled, handoff to Phase 5 execution module to place orders (or to testnet first).

## 5 — LLM usage & prompt design

### 5.1 LLM policy

- **Use LLM for:** summarization, explanation, hypothesis generation, natural-language reformulation of diagnostics.
- **Do not use LLM to** make final trade decisions without human approval.
- All LLM outputs must include `evidence_refs` (IDs of pieces of data used).

### 5.2 Minimal grounding prompt template (pseudo)

```
CONTEXT (structured):
- SYMBOL: ETH/USDT
- QUERY: "Why did ETH scalper fail yesterday?"
- FACTS:
  1) sim_run_id: ... PnL: -2.1% trades: 54, hit_rate: 0.41
  2) model_pred_avg: 0.8% conf:0.62
  3) realized_return: -2.8% (3:00-4:00)
  4) vol_now: 1.9% vs train_vol: 0.8%

INSTRUCTIONS:
- Summarize succinctly (max 200 words).
- Provide 2-3 evidence-backed reasons for failure (each 1 sentence + data ref).
- Provide 2 actionable suggestions (e.g., "increase stop-loss to 1.5%", "pause family").
- Do not invent facts. If evidence is missing, say "insufficient data" and list what is missing.
```

Return JSON-like answer with keys: `summary`, `causes`, `actions`, `evidence_refs`.

## 6 — API endpoints (assistant)

### POST /api/assistant/query

**Body:**

```json
{ "query":"Why did ETH scalper fail yesterday?", "context": {"symbol":"ETH/USDT","date":"2025-11-04"} }
```

**Response:** 

```json
{ "answer_id":"...", "payload": { "summary": "...", "causes":[...], "actions":[...] }, "evidence_refs":[...] }
```

### GET /api/assistant/history?limit=50

Returns recent chat + audit logs.

### GET /api/assistant/recommendations

List active pending recommendations.

### POST /api/assistant/recommendations/{rec_id}/decision

**Body:**

```json
{ "decision":"approve"/"reject"/"modify", "user_notes": "...", "modified_params": {...} }
```

**Response:** decision stored and audit trail created.

## 7 — Frontend scope (Phase 4 UI)

**New / Updated Pages**
- `/assistant` (new) — Chat + recommendation workspace with conversation history, evidence viewer, and approval modals; consumes `POST /api/assistant/query`, `GET /api/assistant/history`, `GET /api/assistant/recommendations`, `POST /api/assistant/recommendations/{rec_id}/decision`.
- `/assistant/evidence/[id]` (new dynamic route) — Displays detailed evidence artifacts (charts, reports) fetched via `GET /api/assistant/evidence/{id}`.
- `/settings/assistant` (new) — Manage LLM provider, redaction rules, approval thresholds via `GET/PUT /api/settings/assistant`.
- Global layout — Adds notification center and assistant shortcut in header.

**Key Components**
- `ChatTranscript`, `MessageBubble`, `AssistantToolbar`, `QuickActions`.
- `RecommendationCard`, `ApprovalModal`, `EvidenceDrawer`, `EvidenceAttachmentList`.
- `NotificationCenter`, `ToastProvider`, `PromptTemplatePreview`.
- `SettingsAssistantForm` with provider selector and token budgeting controls.

**Backend Integrations**
- Chat: `POST /api/assistant/query`, `GET /api/assistant/history` (stream support optional).
- Recommendations: `GET /api/assistant/recommendations`, `POST /api/assistant/recommendations/{id}/decision`, `POST /api/assistant/recommendations/{id}/snooze`.
- Evidence: `GET /api/assistant/evidence/{id}` for attachments, `GET /api/reports/{date}` when referencing historical charts.
- Settings: `GET/PUT /api/settings/assistant`, `POST /api/settings/assistant/test-connection`.
- Audit trail: surface `GET /api/audit/assistant?limit=...` in UI for compliance view.

**UX & QA**
- Chat layout built with shadcn `Sheet`, `ScrollArea`, `Input`, `Button`; conversation grouped by day with role badges.
- Two-step approvals enforce PIN/MFA via modal stepper; accessible focus management and `aria-live` for assistant responses.
- Toast notifications via `useToast`; ensure reduces motion respect prefers-reduced-motion.
- Playwright tests for sending query, approving recommendation, toggling themes, and verifying audit link opens.
- Storybook for `RecommendationCard`, `ApprovalModal`, `ChatTranscript` with mocked SWR providers.

## 8 — Audit & reproducibility

- Store each conversation turn with: `user_text`, `assistant_text`, `retrieved_evidence_refs`, `llm_model_id`, `timestamp`, `user_id`.
- Store all recommendation objects and decisions with timestamps and related `sim_run`/model artifacts.
- Ability to "replay" an assistant response deterministically given the same evidence (use stored prompt + LLM seed if supported).

## 9 — Security & privacy

- **LLM provider:** if external, redact API keys, PII, wallet addresses before sending. Prefer local LLM for privacy.
- **Access control:** assistant endpoints protected by auth (JWT). Role-based access: only owner can approve large trades.
- **Rate limits** on assistant to prevent accidental bursts.

## 10 — Tests & QA

- **Unit tests:** retriever returns precise evidence for targeted queries.
- **Integration:** chat flow uses same evidence set and LLM output follows prompt schema; assert `evidence_refs` included.
- **Security tests:** verify redaction of private data before calls.
- **Acceptance tests:** for 10 sample queries, assistant must return summary and at least one action or insufficient data flag.

## 11 — Monitoring & metrics

- Track assistant metrics: queries per day, average response length, % of recommendations approved, time-to-approval.
- Track LLM costs if using external provider.
- Monitor correctness: periodic re-check of assistant claims vs data to detect hallucinations.

## 12 — Acceptance criteria (Phase 4 done)

- ✅ Chat UI returns grounded responses with evidence references for 90% of test queries.
- ✅ Recommendation objects created automatically and require explicit user approval for live execution.
- ✅ Audit logs store every assistant action + user decision with full evidence.
- ✅ Safety flows (two-step confirmation, pin for large trades) implemented.
- ✅ Tests pass and documentation for prompt templates + retriever functions included.
- ✅ Assistant workspace frontend ships with shadcn components, theme toggle, responsive layouts, and accessible chat/recommendation flows.
- ✅ Settings page persists assistant provider + approval thresholds via `/api/settings/assistant`.

## 13 — Implementation Status (Nov 10, 2025)

**Completed**
- `assistant/schemas.py`, `assistant/repository.py`, `assistant/retriever.py`, `assistant/explainer.py`, `assistant/action_manager.py`, and `assistant/llm_worker.py` now collaborate to retrieve evidence, synthesise grounded answers, log conversations, and curate trade recommendations.
- FastAPI endpoints under `/api/assistant` deliver chat responses, conversation history, evidence lookups, and decision handling; `/api/settings/assistant` exposes persisted LLM + approval preferences with connectivity checks.
- Dashboard evaluation HTML now writes with Windows-safe encoding, keeping em dash headings and replacing the training-window arrow with `&rarr;` to satisfy pytest expectations.

**API Surface**
- `POST /api/assistant/query` accepts contextual queries, returns structured `summary/causes/actions`, and stores audit turns with evidence snapshots.
- `GET /api/assistant/history`, `/recommendations`, and `/evidence/{namespace}/{id}` power the workspace; `POST /api/assistant/recommendations/{rec_id}/decision|snooze` records human feedback.
- Settings router gained `GET/PUT /api/settings/assistant` plus `POST /api/settings/assistant/test-connection`, wiring through `AssistantSettings` storage and `LLMWorker` probes.

**Frontend**
- New Next.js workspace at `/assistant` renders `AssistantToolbar`, `ChatTranscript`, `EvidenceDrawer`, `RecommendationCard`, and `PromptTemplatePreview`; quick prompts and toast notifications encourage conversational triage.
- `/assistant/evidence/[id]` surfaces raw artefacts, while `/settings/assistant` manages provider, grounding, approval, and notification configuration.
- Global layout adds an `Assistant` nav entry, `NotificationCenter`, and app-wide `ToastProvider` context.

**Tooling & Tests**
- Playwright coverage extended via `web/next-app/playwright/assistant.spec.ts`; pytest suite now passes end-to-end with updated directional-accuracy logic and encoding-safe evaluation dashboards.
- Unit tests `tests/test_assistant_service.py` exercise recommendation generation fallbacks; running `pytest` completes without failures (`11 passed, 4 warnings`).

**Known gaps / next**
- UI currently renders approvals synchronously; async streaming + optimistic updates remain future work.
- Evidence retriever leans on Mongo availability; additional caching + vector retrieval could tighten latency when scaling beyond Phase 4.