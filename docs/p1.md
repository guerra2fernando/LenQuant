# Phase 1 ‚Äî Multi-horizon forecast + Ensemble (MVP predictive core)

**Working goal:** Build separate forecasting models for minute, hour, day and wrap them into an ensemble manager that produces a single signal + confidence; integrate forecasts into simulator and reporting.

This doc assumes Phase 0 is complete (data pipeline, features, backtester).

## 1 ‚Äî Objectives

- Train baseline statistical models and a LightGBM/RandomForest per horizon (1m, 1h, 1d, 1M optionally).
- Save model artifacts & metadata (training window, metrics) in DB.
- Create a Forecast API that returns `predicted_return`, `confidence`, `model_version` for a `(symbol, horizon, timestamp)`.
- Add ensemble manager that merges outputs into scoring used by strategies.
- Validate uplift vs naive baseline and produce evaluation reports.
- Deliver a Phase 1 Next.js experience: forecast explorer, model registry views, and ensemble signal overlays built with Tailwind + shadcn, fully responsive in light and dark themes.

## 2 ‚Äî New dependencies

- **lightgbm** or **xgboost** or scikit-learn (GradientBoosting/RandomForest)
- **joblib** / pickle for saving models
- **mlflow** (optional) or a simple model registry collection in MongoDB: `models.registry`
- **dvc** (optional) for dataset versioning

## 3 ‚Äî Data & target construction

For each horizon `h` (e.g., 1m, 1h, 1d):

- **Target:** `future_return_h = (close_{t+h} / close_t) - 1`
- **Classification variant:** `future_up_h = future_return_h > threshold_h` (e.g., 0.001 for 1m)
- **Training windows:** rolling window approach, e.g., train on last N days (configurable per horizon)

Store training metadata:

```json
{
  "model_id": "rf_1m_v1",
  "symbol": "BTC/USDT",
  "horizon": "1m",
  "trained_at": ISODate(...),
  "train_start": ISODate(...),
  "train_end": ISODate(...),
  "metrics": { "rmse": 0.0012, "mae": 0.0009, "dir_acc": 0.56 },
  "artifact_path": "models/rf_1m_v1.joblib"
}
```

## 4 ‚Äî Training pipeline (script + design)

**models/train_horizon.py**

1. Load last WINDOW days of features + prices
2. Build X (features) and y (future_return_h)
3. Split: time-based train/val/test (80/10/10) in time-order
4. Train baseline: ewma (persistence), ARIMA (optional)
5. Train ML: LightGBM or RandomForest
6. Compute metrics: RMSE, MAE, directional accuracy, Brier score (if classification)
7. Save model artifact and registry entry. Save evaluation plots (predicted vs realized) to `reports/model_eval/`.

üî¢ **Implemented:** `models/train_horizon.py` now performs the full workflow above. It supports `--algorithm rf|lgbm`, persists artifacts under `models/artifacts/`, writes evaluation CSVs to `reports/model_eval/`, and registers metadata (feature set, metrics, training window) in MongoDB via `models.registry`.

### Example training command

```bash
python models/train_horizon.py --symbol BTC/USDT --horizon 1m --train-window 90
```

### 4.1 Feature importance & SHAP

For interpretability, store feature importance and lightweight SHAP explainer outputs for the ML model (small sample).

Save summary to DB.

üßæ **Implemented:** Training runs now compute tree-based feature importances and SHAP mean-absolute values, persist the top features in the registry, and write JSON artifacts under `reports/model_eval/{model_id}_shap_summary.json` for UI consumption.

## 5 ‚Äî Forecast serving & API

Add endpoints:

### POST /api/forecast

Payload:

```json
{ "symbol": "BTC/USDT", "timestamp": "2025-11-04T12:00:00Z", "horizon": "1h" }
```

Response:

```json
{ "symbol":"BTC/USDT","horizon":"1h","timestamp":"...","pred_return":0.0123,"conf":0.72,"model_id":"lgbm_1h_v4" }
```

### GET /api/forecast/batch?symbols=BTC/USDT,ETH/USDT&horizon=1h

Returns batch forecasts for UI and simulations.

### GET /api/forecast/export?symbols=BTC/USDT,ETH/USDT&horizon=1h

Streams a CSV snapshot of the latest ensemble predictions (symbol, horizon, timestamp, predicted return, confidence, error) for download and archival.

üõ∞Ô∏è **Implemented:** `api/routes/forecast.py` now powers the single, batch, and export endpoints, handling timestamp parsing, batch fan-out, and returning normalized payloads with prediction, confidence, and contributing model breakdowns.

### Ensemble manager (server-side)

Accepts multiple model outputs per horizon (ARIMA, RF, LGBM) and computes weighted avg:

```
score = Œ£ w_i * pred_i where w_i = function(model_perf_metric) (e.g., inverse RMSE normalized)
```

Compute confidence: derived from model variance + historical calibration (e.g., how often similar predictions were correct).

üßÆ **Implemented:** `models/ensemble.py` loads cached model artifacts per `(symbol, horizon)`, weights predictions by inverse RMSE, and derives a confidence scalar from prediction dispersion.

### 5.1 Frontend scope (Phase 1 UI)

**Shipped Pages**
- `/forecasts` (new) ‚Äî 1m/1h/1d horizon buttons backed by SWR polling of `GET /api/forecast/batch`, CSV export via `GET /api/forecast/export`, and manual refresh.
- `/models/registry` (new) ‚Äî Filterable registry table (`ModelRegistryTable`) consuming `GET /api/models/registry`, detail summary card for metrics/feature importance/SHAP output via `GET /api/models/registry/{model_id}`, and background retrain scheduling with `POST /api/models/retrain`.
- `/settings` ‚Äî Extends Phase 0 preferences with a Mongo-persisted retraining editor (window, cadence, threshold) powered by `GET/PUT /api/settings/models`, adds a single-click Phase 0 bootstrap trigger (`POST /api/admin/bootstrap`), and surfaces bulk retraining controls plus job monitoring backed by `/api/models/retrain/bulk` and `/api/models/retrain/jobs`.

**UI Backlog**
- Home dashboard forecast status card leveraging the new sparkline component.
- Deeper SHAP visualisations (per-feature drill downs, temporal SHAP evolution).

**Key Components**
- `ForecastTable`, `ConfidenceIndicator`, CSV export + refresh actions using shadcn buttons/badges and optimistic loading rows.
- `ModelRegistryTable` with inline selection + retrain action, plus a metrics detail card rendered with shadcn cards.
- Shared `Layout`, `theme-provider`, and Tailwind-based theming carried forward from Phase 0.

**Backend Integrations**
- Forecast APIs: `POST /api/forecast`, `GET /api/forecast/batch`, `GET /api/forecast/export`.
- Model registry: `GET /api/models/registry`, `GET /api/models/registry/{model_id}`, `POST /api/models/retrain`.
- Retraining orchestration: `POST /api/models/retrain/bulk` schedules horizon-wide jobs (logs in `jobs.model_training`), `GET /api/models/retrain/jobs` lists recent runs.
- Settings: `GET/PUT /api/settings/models` to configure retrain cadence per horizon.
- Data bootstrap: `POST /api/admin/bootstrap` (seed symbols + feature refresh) exposed on the settings screen.

üß© **Implemented:** The `settings` FastAPI router persists per-horizon retraining preferences in MongoDB, and `scripts/run_retraining.py` reads those settings to launch horizon-specific training jobs (suitable for cron/Prefect scheduling).

**UX & QA**
- Optimistic loading and skeletons for forecast tables; `aria-live` announcements on refresh.
- Export CSV action hitting `GET /api/forecast/export`.
- Playwright test to ensure `/forecasts` renders across theme modes and the model detail card toggles without console warnings.
- Storybook coverage for `ForecastTable`, `ModelRegistryTable`, and `ConfidenceIndicator`.

## 6 ‚Äî Integrate forecasts into strategies

Modify backtester to accept `predicted return` + `confidence` per tick:

**Example simple rule:** if `pred_return_hour > min_ret_threshold` and `conf > min_conf` ‚Üí signal buy

Position sizing uses predicted return & confidence: larger size when `predicted return √ó conf` bigger.

Add configurable flags to allow the strategy to use only certain horizons (e.g., scalping uses 1m & 5m, swing uses 1d).

Track prediction vs realized on each trade for later model performance analysis.

üïπÔ∏è **Implemented:** `backtester/engine.py` now accepts per-tick `predicted_return` and `confidence`, tracks realized returns on exits, and records prediction metadata in the equity curve. `simulator/runner.py` consumes ensemble forecasts, applies configurable thresholds per horizon, and stores runs with forecast-aware metrics.

## 7 ‚Äî Evaluation & metrics

### Per-model & per-horizon reports:

- Regression metrics: RMSE, MAE
- Directional accuracy (hit rate)
- Calibration plots (predicted vs realized buckets)
- Profitability of trades that used model signals vs baseline strategies
- **Uplift metric:** `(strategy_pnl_with_model / strategy_pnl_baseline) - 1`

Persist evaluation payloads under `reports/model_eval/` (CSV predictions, SHAP summary JSON, HTML dashboards) and record artifact paths in `models.registry`.

### 7.1 Evaluation dashboards & uplift tooling

- `reports/evaluation_dashboard.py` renders `reports/model_eval/{model_id}_dashboard.html` during training, combining key metrics, SHAP feature bars, and the latest prediction snippets.
- `reports/uplift_analyzer.py` compares ensemble runs vs baseline simulations directly from MongoDB to quantify uplift:

```bash
python reports/uplift_analyzer.py --symbol BTC/USDT --horizon 1h --ensemble-strategy ensemble-threshold --baseline-strategy baseline
```

Sample output:

```
Uplift analysis:
  ensemble_run_id: run-20251109-ensemble
  baseline_run_id: run-20251108-baseline
  pnl_uplift: 0.073
  sharpe_delta: 0.42
```

üìä **Implemented:** Each training run exports a CSV of test predictions to `reports/model_eval/{model_id}_test.csv` for quick inspection and persists metrics (RMSE, MAE, directional accuracy) alongside artifact metadata in MongoDB.

## 8 ‚Äî Model retraining & scheduling

- **Retrain cadence:** configurable per horizon (1m models retrain daily; 1d models weekly)
- Implement training job scheduling with Celery/cron/Prefect.
- Implement "auto-promote" logic: promote new model if validation metrics improved and forward-backtest shows uplift.

üóìÔ∏è **Implemented:** `scripts/run_retraining.py` acts as the Phase 1 scheduler entrypoint‚Äîreading `/api/settings/models` preferences and invoking `models.train_horizon` for each configured symbol/horizon pair. Hook it into cron/Prefect to achieve the desired cadence today.

## 9 ‚Äî Model governance

- `models.registry` collection in MongoDB with `model_id`, metrics, artifact path, `status` (candidate, production, deprecated).
- Rollbacks can be performed by updating `status` via `models.registry` helpers (`registry.update_model_status`); a dedicated admin endpoint remains a backlog improvement.

## 10 ‚Äî Example config & hyperparameters

**Default horizons config**

The runtime defaults live in `models/train_horizon.py::DEFAULT_CONFIG`. Persisting them to YAML is optional; an example structure is below for future externalisation:

```yaml
horizons:
  - name: 1m
    lookahead: 1
    train_window_days: 30
    retrain_cadence: daily
    threshold_pct: 0.001
  - name: 1h
    lookahead: 60
    train_window_days: 180
    retrain_cadence: daily
    threshold_pct: 0.01
```

## 11 ‚Äî Code snippets

### Training (LightGBM pseudo)

```python
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit
X_train, y_train = ...
dtrain = lgb.Dataset(X_train, label=y_train)
params = {'objective':'regression','metric':'rmse','learning_rate':0.05}
bst = lgb.train(params, dtrain, num_boost_round=500, valid_sets=[dval], early_stopping_rounds=50)
joblib.dump(bst, 'models/lgbm_1h_v1.joblib')
```

### Serving (ensemble)

```python
def ensemble_predict(symbol, timestamp, horizon):
    preds = []
    for m in get_models(symbol,horizon):
        model = load(m.artifact_path)
        feat = load_features(symbol,timestamp)
        p = model.predict(feat)
        preds.append({'pred': p, 'rmse': m.metrics['rmse']})
    # weight inversely by rmse
    weights = [1/(p['rmse']+1e-9) for p in preds]
    norm = sum(weights)
    ensemble_pred = sum(w*p['pred'] for w,p in zip(weights,preds))/norm
    conf = 1/(np.std([p['pred'] for p in preds])+1e-6)
    return ensemble_pred, conf
```

## 12 ‚Äî Tests

- **Unit test:** Ensure training pipeline produces expected model artifact and registry entry.
- **Integration test:** Given a fixed seed dataset, the ensemble API returns consistent predictions; Next.js `/forecasts` page renders forecast tables in both light and dark themes without console errors (Playwright smoke).
- **Backtest experiment:** Run strategy using ensemble signals for a 30-day window and compare PnL with baseline (persistence model).

## 13 ‚Äî Acceptance criteria (Phase 1 done)

- ‚úÖ Working ML pipeline trains at least one ML model per horizon (1m, 1h, 1d).
- ‚úÖ Forecast API responds with `pred_return`, `confidence` and `model_id`.
- ‚úÖ Ensemble manager exists and used by simulator/backtester.
- ‚úÖ Backtest shows measurable uplift vs naive baseline on at least one symbol (documented).
- ‚úÖ Retraining jobs scheduled and model registry working.
- ‚úÖ Forecast Studio UI (forecasts + model registry) shipped with Tailwind + shadcn components, theme toggle, and accessible states.
- ‚úÖ Settings page saves per-horizon retraining preferences via `/api/settings/models`.

## 14 ‚Äî Implementation Status (Nov 10, 2025)

**Completed**
- Training CLI (`models/train_horizon.py`) with registry + evaluation artifact exports (CSV + SHAP summaries).
- Mongo-backed model registry helpers (`models/registry.py`) and ensemble inference (`models/ensemble.py`) wired into FastAPI.
- Forecast endpoints (`/api/forecast`, `/api/forecast/batch`, `/api/forecast/export`) and model registry endpoints (`/api/models/registry`, `/api/models/registry/{id}`, `/api/models/retrain`).
- Bulk retraining orchestration via FastAPI (`/api/models/retrain/bulk`, `/api/models/retrain/jobs`) wrapping the horizon-aware CLI, logging job metadata in Mongo for frontend monitoring.
- Backtester + simulator upgraded to ingest ensemble predictions, confidence thresholds, and log forecast-aware metrics end-to-end.
- Forecast Studio Next.js screens (`/forecasts`, `/models/registry`, `/settings`) consuming the new APIs with shadcn tables, filters, retrain actions, CSV export, SWR-powered polling, data bootstrap actions, retraining job history, and per-symbol trend sparklines + SHAP progress bars.
- Automated evaluation dashboards rendered to HTML for every trained model (`reports/evaluation_dashboard.py`) and uplift CLI to compare ensemble vs baseline runs (`reports/uplift_analyzer.py`).
- GitHub Actions CI pipeline (`.github/workflows/ci.yml`) running pytest unit tests and Playwright smoke tests; backend unit coverage for ensemble weighting, training utilities, and dashboard generation added in `tests/`.
- Model settings API (`/api/settings/models`) + `scripts/run_retraining.py` bridge retraining preferences to scheduled jobs; training metadata now captures feature importances and SHAP summaries for UI display.
- Docker Compose stack (`docker/docker-compose.yml`) runs MongoDB + FastAPI + Next.js with Phase 1 services enabled.

**In Progress**
- None ‚Äî Phase 1 backlog items have been delivered. Planning transitions to Phase 2 (strategy genome experimentation).

**Next Up**
- Prepare the Phase 2 roadmap leveraging uplift insights and automated reporting.
- Extend the home dashboard with forecast status cards that reuse the new sparkline component.
- Evaluate managed schedulers (Celery/Prefect) for production-grade retraining orchestration beyond cron scripts.

## 15 ‚Äî Docker workflow
- `docker/docker-compose.yml` runs the full stack: MongoDB, FastAPI (training + forecast APIs, settings, retraining helpers), and the Next.js Forecast Studio.
- From the repository root run `docker compose -f docker/docker-compose.yml up --build` (or `... up --build -d` for detached) to launch the Phase 0/1 environment. The compose file sets `MONGO_URI=mongodb://mongo:27017/lenxys-trader` for the API container and `NEXT_PUBLIC_API_URL=http://api:8000` for the web container.
- API is available at `http://localhost:8000`, Next.js UI at `http://localhost:3000`, and MongoDB at `mongodb://localhost:27017`. Use `docker compose down` to stop the stack and `docker compose logs -f api` to tail training/forecast logs.
