
# Lenxys Trader Questionnaire Responses

## Q1 · Direct Arbitrage Capability
Lenxys Trader already has the plumbing needed for cross-exchange latency arbitrage even though the dedicated Phase 10 orchestrator has not been shipped yet. Real-time prices are pulled through the shared exchange layer: `CCXTConnector.get_orderbook` provides a live best bid/ask snapshot per exchange and the connector cache inside `OrderManager` keeps a warm ccxt session per venue so we do not pay connection setup costs mid-signal. A cross-exchange scout can therefore grab the top of book for Binance Futures and (for example) OKX, compute the spread ΔP in microseconds, and hand it to the existing risk path.

Before any legs are sent, `RiskManager.pre_trade_check` takes the intended notional and applies every guard rail: per-mode caps, symbol exposure, open exposure, daily loss, and the max slippage percentage that defaults to 2%. The arbitrage engine would approve only if the gross spread comfortably exceeds the tolerant loss window. For the 5% of a 1,000 USD account mentioned in Syntax Flux (a 50 USD position), the current configuration requires ΔP to clear roughly 1 USD (2% of notional) plus taker fees and funding to be net-positive. That gives us an immediate “profitable-or-not” decision tied directly to the state stored in Mongo. Once approved, `OrderManager` fires two orders by calling the cached connectors: buy on the discounted exchange, sell on the overpriced one. Every execution is stored in `trading_orders` / `trading_fills`, and a ledger snapshot is hashed by `SettlementEngine` for audit.

Because true dual-exchange arbitration is the next milestone, today you trigger it through a Celery worker or script that reads the order books, evaluates ΔP, and then posts two orders through the documented `/api/trading/orders` endpoint. The frontend is already ready for this flow: configure both exchange connections under **Settings → Trading**, turn on the relevant modes, and monitor the fills in the `/trading` workspace where the “Execution Latency” widget clarifies if the hedge is keeping pace. When Phase 10 lands, the only change will be replacing the external script with the native arbitrage coordinator; all the guard rails, storage, and UI you need are already live.

## Q2 · Low-Latency Execution
The system tracks order lifecycle timestamps automatically. `OrderManager._persist_order` stores `created_at` and `updated_at` for every submission, while fills record their execution time. The `ExecutionLatencyChart` on the `/trading` dashboard simply computes the delta and overlays average versus peaks once you have a few fills. In the current paper and testnet runs the median sits around the configured simulator delay (the default 250 ms in `PaperSettings.latency_ms`). Real exchange metrics will depend on your hosting, but the instrumentation is ready: once you point the live connector at Binance Futures the chart and the `trading_metrics` collection will immediately start reporting your actual round trips.

Three mechanisms keep the latency footprint low today. First, `OrderManager` caches one ccxt client per mode so we reuse the underlying HTTP session (effectively a light connection pool). Second, we keep quote lookups to a single depth-1 call (`estimate_price`) to avoid heavy book downloads before placing an order. Third, the trading API is synchronous by design and runs in the same FastAPI process as the connectors, so there is no cross-service hop on the hot path. For genuine sub-millisecond aspirations you would deploy close to the exchange and replace ccxt’s REST call with the future WebSocket/L3 listener that Phase 10 will bring, but the present implementation already gives you visibility, caching, and batching knobs to diagnose where extra milliseconds come from. On the frontend you can toggle to the advanced theme, open **Trading → Latency**, and watch the live bars update; adjusting the simulated latency (for paper) or confirming live median latency is as simple as editing the `SettingsTradingForm` control and saving.

## Q3 · Order Book Depth & Slippage
The pre-trade checks revolve around the notional you are about to send and whether the top of book can cover it. When you preview or submit an order, `OrderManager` estimates the price by reading a depth-one order book via `connector.get_orderbook`. That price multiplied by the requested quantity produces the notional that `RiskManager.pre_trade_check` evaluates. If you attempt to take 5% of a 1,000 USD account with 20× leverage, the notional the risk engine sees is 50 USD (position size) multiplied by the leverage factor you supplied in the order payload. The guard rails then enforce the configured limits: `max_trade_usd`, `max_open_exposure_usd`, symbol-specific caps, and the global `max_slippage_pct`.

Because the system assumes the first price level can absorb the trade, the implied depth requirement is simple: the sum of the first-level quantities on each exchange must exceed the leveraged quantity you are about to trade. With the built-in defaults (2% max slippage), a 50 USD position can give up at most 1 USD to unfavorable micro-moves; if the live ΔP is any smaller, the trade is rejected. In practice you can tighten this tolerance by lowering `max_slippage_pct` under **Settings → Trading**; the frontend form writes directly into `trading_settings.risk.max_slippage_pct`, so your new cap is immediately used by `risk_manager`. A production upgrade on the roadmap is to sample a deeper book (limit=5) and sum the volumes to guarantee that the entire leveraged quantity is available, but even without that the engine already blocks oversize orders, limits slippage, and triggers manual approval whenever the notional crosses the “sensitive threshold,” keeping thin-margin arbitrage from overrunning liquidity.

## Q4 · Systematic Leverage Scaling & Circuit Breaker
Leverage scaling is data-driven: every fill flows through `RiskManager.record_fill`, which increments the `trading_metrics` document keyed by day. A scheduled task (for example the existing Celery service in `manager/tasks.py`) reads yesterday’s realized P&L, and if it is positive it writes the next leverage tier into `TradingSettings.modes["live"].default_leverage` by calling `RiskManager.update_settings`. The recommended policy is a simple lookup ladder (5× → 10× → 50×) that only advances when the last 24-hour P&L is positive and never skips a step. Because the settings are persisted in Mongo, the trading engine will automatically start using the higher leverage on the next order without restarting services.

Drawdown protection is already coded: `RiskManager._daily_realized_loss` monitors realized P&L, and if the loss breaches the configured cap it throws a `RiskViolation`. To translate LULU’s -10% rule into the existing structure you keep the daily loss limit aligned with 10% of starting equity and use a short loop: (1) fetch today’s loss from `RiskManager.get_summary`, (2) if it is ≤ -10% call `RiskManager.trigger_kill_switch` to halt trading and cancel open orders, (3) reset leverage to the base tier by updating `TradingSettings` back to 5×, and (4) log the breach through `risk_manager.log_breach` so the `/risk` dashboard highlights it. On the frontend, you review leverage status and kill switch state in the **Risk** workspace and adjust the ramp schedule in **Settings → Trading** where leverage and kill-switch options live; no manual code changes are needed.

## Q5 · Overfitting Detection & Automated Response
The overfitting guard lives in `monitor.overfit_detector.detect_overfit`. Every learning cycle pulls the latest `sim_runs`, groups them by strategy, and calculates how sharply the recent ROI and Sharpe ratios have decayed versus the historical baseline. By default we examine a six-run window and flag anything that loses more than 35% of its ROI or sees a Sharpe drop beyond the threshold. When a decay is spotted, the detector emits an `OverfitSignal` with the strategy id, decay score, and run history; `learning.loop.run_learning_cycle` persists those signals through `record_overfit_alerts`, surfaces them in the `/insights` UI, and tags the strategy so it no longer shows up in the champion pool.

The response is two-fold. First, the Bayesian optimizer (`learning.bayes_optimizer.optimise_genomes`) only samples parents from the “champion” list, so once a strategy is flagged it is excluded until you acknowledge and rehabilitate it. Second, you can dial down exploration rates or mutation ranges via **Settings → Learning** — that form writes into `learning.repository` and lets you shrink `exploration_weight` or reduce the number of trials automatically as soon as overfit alerts start piling up. This keeps live trading from favouring stale arbitrage genomes and directs compute toward healthier branches. In the UI, the Learning Insights page shows open alerts, lets you acknowledge them, and details the decay so even non-technical operators can see why a strategy was benched.

## Q6 · 24/7 Operation & Proof-of-Execution
Continuous service is achieved by keeping all mission-critical work in background tasks and storing tamper-evident snapshots on every fill. Order placement, fills, and reconciliations all share one Mongo database, and Celery (`manager/tasks.py`) schedules daily cycles: strategy experiments, evolution, learning, and the reconciliation job. The reconciliation API (`/api/trading/reconciliation`) wraps `SettlementEngine.reconciliation_report`, which compares internal ledgers to exchange balances for each mode. If discrepancies appear, alerts are raised through `monitor.trade_alerts` so you can intervene before the next session.

Proof-of-execution happens per trade. `SettlementEngine.record_fill` is invoked each time an order fills; it writes the fill, updates wallet balances, recalculates realized and unrealized P&L, and creates a hashed ledger snapshot that is stored in `trading_ledgers`. `TradeAuditor.record_event` then hashes the entire order event (including the risk context) and stores it in `trading_audit`. Because the hashes are chained per fill, you can prove that internal P&L reconciles with the stored balances at any time. The frontend exposes all of this in the **Trading** and **Risk** views: live positions, fills, ledger hashes, and reconciliation state update continuously via polling or the `/ws/trading` stream, so operators have round-the-clock visibility without leaving the dashboard.

## Q7 · Directional Shift at 5,000 USD
When capital crosses the 5,000 USD mark you can lean on the existing forecast stack to build a single high-conviction trade. `models/ensemble.ensemble_predict` already consolidates multiple 1-hour and 1-day models by weighting predictions with inverse RMSE and turning the dispersion into a confidence score. `simulator.runner._decide_signal` demonstrates the decision rule: if the weighted prediction for the hour or day horizon exceeds the configured return threshold and confidence surpasses the minimum, it produces a “buy” or “sell”.

To run the Day 15/16 playbook you configure the directional strategy genome (via the Evolution Lab or Learning Insights) to require agreement between the 1h and 1d ensemble outputs and set the target leverage to 10× in the strategy parameters. Automatic Mode already stops anything below the configured confidence — the default threshold is 0.65 (`TradingSettings.auto_mode.confidence_threshold`), so Automatic Mode will only execute the large trade when both horizons jointly deliver at least that confidence. If you want a stricter bar you can raise the threshold in **Settings → Trading → Automatic Mode**. In practice, once the assistant recommends the directional position you approve it (or let auto-mode execute if thresholds are satisfied) and monitor it from the `/trading` page; confidence and expected return show up in the order metadata so everyone understands the rationale.

## Q8 · Profit Reallocation Workflow
Profit reallocation is still in the planning queue. The platform already maintains the figures you need: `SettlementEngine` tracks realized profit per mode, and the ledger snapshots make it trivial to compute “earned profit versus base capital”. What is missing is the final leg that calls the exchange withdrawal API. To implement the 50% off-exchange rule safely you would extend the connector layer with a dedicated withdrawal method, store withdrawal-only API keys (ccxt requires the `withdraw` permission), and bolt a manual approval flow on top so a second human confirms the transfer. Until that lands, the recommended workaround is to use the ledger report (`/api/trading/reconciliation`) to identify realized profit and perform the transfer manually on the exchange, recording it in the audit log for traceability. The frontend already has the right entry points — a future update to **Settings → Trading** or a new “Treasury” tab can host the workflow once the withdrawal hooks are coded.

## Q9 · LLM Decision Rationale with SHAP
The assistant’s explanations are grounded in stored evidence. When a high-leverage trade executes, `assistant/action_manager` attaches the strategy id, run id, and risk metadata to the recommendation. Those ids let the retriever pull SHAP summaries saved during model training (`reports/model_eval/{model_id}_shap_summary.json`) and any trade-level metrics in `sim_runs`. The explainer in `assistant/explainer.py` then composes a context document that lists each evidence item and feeds it to the selected LLM. Because SHAP values are part of the evidence payload, the model can point to drivers such as “1h RSI crossing up” or “VWAP gap widening”.

An example explanation the assistant would generate after a +150% ROI trade might read like this: “We bought BTC/USDT on Binance with 50× leverage because the 1h ensemble predicted +1.8% (confidence 0.81) while the 1d ensemble confirmed the direction. Top SHAP contributors were `rsi_14` (+0.0072), `vwap_deviation` (+0.0049), and `funding_rate_delta` (+0.0031), meaning momentum, price relative to VWAP, and favorable funding jointly drove the signal. Exit was triggered when RSI normalized and VWAP spread closed, locking +150% on the 2% margin allocated.” The assistant UI shows the summary, the SHAP contributor list, and the evidence references in an expandable drawer so reviewers can inspect the raw charts without reading code.

## Q10 · ccxt Exchange Management Resilience
Each trading mode owns its own connector instance, so an OKX outage does not leak into Binance. `OrderManager._ensure_connector` caches one connector per mode; if the OKX ccxt client throws an exception on initialization or order placement, the failure is caught, logged via `risk_manager.log_breach`, and the Binance connector stays untouched in the cache. Orders routed to Binance continue flowing because they go through a different `ModeSettings` entry and a different ccxt object. The frontend mirrors that separation: the `/trading` tabs split Paper, Testnet, and Live accounts, and the risk dashboard highlights per-mode breaches, letting you disable only the failing venue with the kill switch. In short, ccxt isolation plus per-mode settings give you operational resilience without special casing.

## Q11 · “Maximize in 10/30 Days” Requests
You can ask exactly that through the assistant today. The conversational endpoint (`POST /api/assistant/query`) passes your question into the retriever, which pulls the latest evolution metrics, forecast confidence, and account settings. From there, `assistant/action_manager.auto_generate_recommendations` can build a sequence of pending trades aligned with your horizon: for a 10-day window it favours short-term genomes (1h, 4h), while for 30 days it leans on 1d forecasts and the allocator weights stored in `learning.allocations`. The assistant will describe the plan, including how much capital to allocate, what confidence thresholds apply, and which guard rails remain in place.

Execution stays supervised. If you leave Auto Mode off, the recommendations arrive in the **Assistant** dashboard for you to approve. If Auto Mode is on and the confidence threshold is hit, the assistant hands them straight to `OrderManager` with your configured size cap. Either way, every trade runs through the same risk checks and audit trail described above. That means the “maximize over 10 or 30 days” feature is effectively a templated conversation plus the existing scheduled learning cycle, giving non-technical users a clear roadmap without bypassing safety mechanisms.